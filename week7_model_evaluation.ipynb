{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a623a96-25bb-4c30-b450-3d40d693fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 7: Model Evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, confusion_matrix, classification_report,\n",
    "                           roc_curve, roc_auc_score, precision_recall_curve)\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Week 7 Model Evaluation Environment Ready!\")\n",
    "\n",
    "# Load your cleaned dataset\n",
    "df = pd.read_csv('titanic_cleaned.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Load trained models from Week 6\n",
    "try:\n",
    "    lr_model = joblib.load('logistic_regression_model.pkl')\n",
    "    rf_model = joblib.load('random_forest_model.pkl')\n",
    "    dt_model = joblib.load('decision_tree_model.pkl')\n",
    "    print(\"‚úÖ All models loaded successfully!\")\n",
    "except:\n",
    "    print(\"‚ùå Models not found. Please run Week 6 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ab9f4-0cd0-4784-bdfb-a69cc81da090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PREPARING DATA FOR EVALUATION ===\")\n",
    "\n",
    "# Use the same features and preprocessing as Week 6\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target_variable = 'Survived'\n",
    "\n",
    "# Prepare feature matrix X\n",
    "X = df[features].copy()\n",
    "\n",
    "# Handle categorical variables (same encoding as Week 6)\n",
    "X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n",
    "embarked_encoded = pd.get_dummies(X['Embarked'], prefix='Embarked')\n",
    "X = pd.concat([X, embarked_encoded], axis=1)\n",
    "X = X.drop('Embarked', axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Target variable\n",
    "y = df[target_variable]\n",
    "\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "print(f\"Target: {target_variable}\")\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "\n",
    "# Create train-test split (same as Week 6 for consistency)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"Class distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cfca2-93e0-4f6c-a189-755fec7c4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== GENERATING PREDICTIONS ===\")\n",
    "\n",
    "# Get predictions and probabilities from all models\n",
    "models = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'Decision Tree': dt_model\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    probabilities[name] = model.predict_proba(X_test)[:, 1]  # Probability of class 1 (Survived)\n",
    "    print(f\"‚úÖ {name}: Predictions and probabilities generated\")\n",
    "\n",
    "print(f\"\\nSample probability outputs (first 5 passengers):\")\n",
    "sample_probs = pd.DataFrame({name: probs[:5] for name, probs in probabilities.items()})\n",
    "display(sample_probs.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2badc7b5-306f-4e5a-a414-6299d4d7c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CONFUSION MATRIX ANALYSIS ===\")\n",
    "\n",
    "# Create confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    y_pred = predictions[name]\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                xticklabels=['Not Survived', 'Survived'],\n",
    "                yticklabels=['Not Survived', 'Survived'])\n",
    "    \n",
    "    axes[i].set_title(f'Confusion Matrix - {name}', fontweight='bold', fontsize=12)\n",
    "    axes[i].set_xlabel('Predicted Label')\n",
    "    axes[i].set_ylabel('True Label')\n",
    "    \n",
    "    # Calculate metrics from confusion matrix\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    print(f\"\\nüìä {name} Confusion Matrix Analysis:\")\n",
    "    print(f\"   True Negatives (TN): {TN} - Correctly predicted non-survivors\")\n",
    "    print(f\"   False Positives (FP): {FP} - Non-survivors incorrectly predicted as survivors\")\n",
    "    print(f\"   False Negatives (FN): {FN} - Survivors incorrectly predicted as non-survivors\")\n",
    "    print(f\"   True Positives (TP): {TP} - Correctly predicted survivors\")\n",
    "    \n",
    "    # Derived metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    \n",
    "    print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   Precision: {precision:.3f}\")\n",
    "    print(f\"   Recall: {recall:.3f}\")\n",
    "    print(f\"   Specificity: {specificity:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç CONFUSION MATRIX INTERPRETATION GUIDE:\")\n",
    "print(\"‚Ä¢ Ideal: High TP and TN, Low FP and FN\")\n",
    "print(\"‚Ä¢ High FP: Too many false alarms\")\n",
    "print(\"‚Ä¢ High FN: Missing too many actual survivors\")\n",
    "print(\"‚Ä¢ Balance depends on business context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528eb8e8-4ab4-40d7-aa00-eaa1c1e2ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ROC CURVE AND AUC ANALYSIS ===\")\n",
    "\n",
    "# Create ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot ROC curve for each model\n",
    "for name, prob in probabilities.items():\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, prob)\n",
    "    auc_score = roc_auc_score(y_test, prob)\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2, \n",
    "             label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "# Plot random classifier line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall/Sensitivity)', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontweight='bold', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç ROC CURVE INTERPRETATION:\")\n",
    "print(\"‚Ä¢ Perfect classifier: Top-left corner (AUC = 1.0)\")\n",
    "print(\"‚Ä¢ Random classifier: Diagonal line (AUC = 0.5)\")\n",
    "print(\"‚Ä¢ Good classifier: Curve towards top-left (AUC > 0.7)\")\n",
    "print(\"‚Ä¢ Better models have higher AUC values\")\n",
    "\n",
    "# Display AUC scores\n",
    "print(\"\\nüìä AUC SCORES COMPARISON:\")\n",
    "auc_scores = {}\n",
    "for name, prob in probabilities.items():\n",
    "    auc_score = roc_auc_score(y_test, prob)\n",
    "    auc_scores[name] = auc_score\n",
    "    print(f\"{name}: {auc_score:.3f}\")\n",
    "\n",
    "best_auc_model = max(auc_scores, key=auc_scores.get)\n",
    "print(f\"\\nüéØ Best AUC: {best_auc_model} ({auc_scores[best_auc_model]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5e253-3432-4fb9-99fb-8a5747e08833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ROC CURVE WITH THRESHOLD ANALYSIS ===\")\n",
    "\n",
    "# Analyze one model in detail (Random Forest as example)\n",
    "model_name = 'Random Forest'\n",
    "y_prob = probabilities[model_name]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "# Create detailed ROC analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot ROC curve with some threshold points\n",
    "ax1.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc_scores[model_name]:.3f})')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "# Mark some threshold points\n",
    "threshold_points = [0.2, 0.5, 0.8]\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, threshold in enumerate(threshold_points):\n",
    "    # Find closest threshold in the array\n",
    "    idx = np.argmin(np.abs(thresholds - threshold))\n",
    "    ax1.scatter(fpr[idx], tpr[idx], color=colors[i], s=100, \n",
    "               label=f'Threshold = {threshold:.1f}')\n",
    "\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title(f'ROC Curve - {model_name}', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot threshold analysis\n",
    "ax2.plot(thresholds, tpr, label='True Positive Rate (Recall)', linewidth=2)\n",
    "ax2.plot(thresholds, 1 - fpr, label='True Negative Rate (Specificity)', linewidth=2)\n",
    "ax2.set_xlabel('Classification Threshold')\n",
    "ax2.set_ylabel('Rate')\n",
    "ax2.set_title('Threshold Analysis', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç THRESHOLD ANALYSIS INSIGHTS:\")\n",
    "print(\"‚Ä¢ Lower threshold: Higher recall, lower precision\")\n",
    "print(\"‚Ä¢ Higher threshold: Higher precision, lower recall\")\n",
    "print(\"‚Ä¢ Choose threshold based on business requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7307c-a832-4cdb-89cf-7ce0ebc97fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMPREHENSIVE METRICS CALCULATION ===\")\n",
    "\n",
    "# Calculate multiple evaluation metrics for all models\n",
    "metrics_results = []\n",
    "\n",
    "for name in models.keys():\n",
    "    y_pred = predictions[name]\n",
    "    y_prob = probabilities[name]\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    # Additional metrics from confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    specificity = TN / (TN + FP)  # True Negative Rate\n",
    "    false_positive_rate = FP / (FP + TN)  # Fallout\n",
    "    negative_predictive_value = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "    \n",
    "    metrics_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1,\n",
    "        'AUC': auc,\n",
    "        'Specificity': specificity,\n",
    "        'False_Positive_Rate': false_positive_rate,\n",
    "        'True_Positives': TP,\n",
    "        'False_Positives': FP,\n",
    "        'True_Negatives': TN,\n",
    "        'False_Negatives': FN\n",
    "    })\n",
    "\n",
    "# Create comprehensive results dataframe\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "metrics_df = metrics_df.sort_values('F1_Score', ascending=False)\n",
    "\n",
    "print(\"üìä COMPREHENSIVE EVALUATION METRICS:\")\n",
    "display(metrics_df.round(3))\n",
    "\n",
    "# Save detailed metrics\n",
    "metrics_df.to_csv('comprehensive_metrics.csv', index=False)\n",
    "print(\"‚úÖ Comprehensive metrics saved to 'comprehensive_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ace5d-90a9-4992-a89b-a40edfac48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PRECISION-RECALL CURVE ANALYSIS ===\")\n",
    "\n",
    "# Create Precision-Recall curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, prob in probabilities.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, prob)\n",
    "    \n",
    "    # Calculate area under precision-recall curve\n",
    "    pr_auc = np.trapz(precision, recall)\n",
    "    \n",
    "    plt.plot(recall, precision, linewidth=2,\n",
    "             label=f'{name} (PR AUC = {pr_auc:.3f})')\n",
    "\n",
    "# Add baseline (percentage of positive class)\n",
    "baseline = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.axhline(y=baseline, color='red', linestyle='--', \n",
    "           label=f'Baseline (Positive Rate = {baseline:.3f})')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curves', fontweight='bold', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç PRECISION-RECALL CURVE INTERPRETATION:\")\n",
    "print(\"‚Ä¢ Perfect classifier: Top-right corner\")\n",
    "print(\"‚Ä¢ Baseline: Horizontal line at positive class rate\")\n",
    "print(\"‚Ä¢ Good classifier: Curve towards top-right\")\n",
    "print(\"‚Ä¢ Particularly useful for imbalanced datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecace45-a0f8-4ca8-b651-300c5c5b75ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== METRIC TRADE-OFF ANALYSIS ===\")\n",
    "\n",
    "# Create visualization showing trade-offs between metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Precision vs Recall\n",
    "for i, row in metrics_df.iterrows():\n",
    "    axes[0, 0].scatter(row['Recall'], row['Precision'], s=100, label=row['Model'])\n",
    "    axes[0, 0].text(row['Recall'] + 0.01, row['Precision'] + 0.01, row['Model'], fontsize=9)\n",
    "\n",
    "axes[0, 0].set_xlabel('Recall')\n",
    "axes[0, 0].set_ylabel('Precision')\n",
    "axes[0, 0].set_title('Precision vs Recall Trade-off', fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "axes[0, 0].set_xlim(0.5, 1)\n",
    "axes[0, 0].set_ylim(0.5, 1)\n",
    "\n",
    "# Plot 2: F1-Score vs AUC\n",
    "for i, row in metrics_df.iterrows():\n",
    "    axes[0, 1].scatter(row['AUC'], row['F1_Score'], s=100, label=row['Model'])\n",
    "    axes[0, 1].text(row['AUC'] + 0.01, row['F1_Score'] + 0.01, row['Model'], fontsize=9)\n",
    "\n",
    "axes[0, 1].set_xlabel('AUC')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].set_title('F1-Score vs AUC', fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xlim(0.7, 0.9)\n",
    "axes[0, 1].set_ylim(0.6, 0.8)\n",
    "\n",
    "# Plot 3: Sensitivity vs Specificity\n",
    "for i, row in metrics_df.iterrows():\n",
    "    axes[1, 0].scatter(row['Specificity'], row['Recall'], s=100, label=row['Model'])\n",
    "    axes[1, 0].text(row['Specificity'] + 0.01, row['Recall'] + 0.01, row['Model'], fontsize=9)\n",
    "\n",
    "axes[1, 0].set_xlabel('Specificity')\n",
    "axes[1, 0].set_ylabel('Sensitivity (Recall)')\n",
    "axes[1, 0].set_title('Sensitivity vs Specificity', fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].set_xlim(0.7, 0.95)\n",
    "axes[1, 0].set_ylim(0.5, 0.9)\n",
    "\n",
    "# Plot 4: Metric radar chart preparation\n",
    "metrics_for_radar = ['Accuracy', 'Precision', 'Recall', 'F1_Score', 'AUC']\n",
    "radar_data = metrics_df[['Model'] + metrics_for_radar].set_index('Model')\n",
    "\n",
    "# Normalize for radar chart\n",
    "radar_data_normalized = radar_data.copy()\n",
    "for col in metrics_for_radar:\n",
    "    radar_data_normalized[col] = radar_data[col] / radar_data[col].max()\n",
    "\n",
    "# Simple bar chart instead of radar for clarity\n",
    "radar_data.plot(kind='bar', ax=axes[1, 1], width=0.8)\n",
    "axes[1, 1].set_title('Model Performance Across Metrics', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç TRADE-OFF ANALYSIS INSIGHTS:\")\n",
    "print(\"‚Ä¢ No single model dominates all metrics\")\n",
    "print(\"‚Ä¢ Different models excel in different aspects\")\n",
    "print(\"‚Ä¢ Choice depends on which errors are more costly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9f6bd-fdf6-460d-8bbd-61e4945ff513",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== BUSINESS CONTEXT ANALYSIS ===\")\n",
    "\n",
    "print(\"üéØ TITANIC SURVIVAL PREDICTION - BUSINESS CONTEXT\")\n",
    "print(\"Understanding the cost of different types of errors:\\n\")\n",
    "\n",
    "# Analyze confusion matrix costs\n",
    "for name in models.keys():\n",
    "    y_pred = predictions[name]\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  False Positives ({FP}): Predicting survival for those who died\")\n",
    "    print(f\"    ‚Üí Cost: Unnecessary hope/false information to families\")\n",
    "    print(f\"  False Negatives ({FN}): Predicting death for survivors\")\n",
    "    print(f\"    ‚Üí Cost: Missing actual survivors, potential rescue opportunities\")\n",
    "    print(f\"  True Positives ({TP}): Correctly identifying survivors\")\n",
    "    print(f\"    ‚Üí Benefit: Accurate information, proper recognition\")\n",
    "    print(f\"  True Negatives ({TN}): Correctly identifying non-survivors\")\n",
    "    print(f\"    ‚Üí Benefit: Accurate historical record\")\n",
    "\n",
    "print(\"\\nü§î CRITICAL QUESTIONS:\")\n",
    "print(\"‚Ä¢ Which error is more serious in this context?\")\n",
    "print(\"‚Ä¢ Is it worse to give false hope or miss actual survivors?\")\n",
    "print(\"‚Ä¢ What are the ethical considerations?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d841069-8fd9-4782-b8b2-0d916dec9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìä WEEK 7 ASSIGNMENT: METRIC REFLECTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nü§î REFLECTION: 'WHICH METRIC IS MOST IMPORTANT FOR MY PROJECT AND WHY?'\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Display key metrics for reference\n",
    "print(\"\\nüìà KEY METRICS SUMMARY:\")\n",
    "key_metrics = metrics_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'AUC']]\n",
    "display(key_metrics.round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REFLECTIVE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ PROJECT CONTEXT: Titanic Survival Prediction\n",
    "\n",
    "This project aims to predict passenger survival on the Titanic based on \n",
    "pre-voyage characteristics. The context is historical analysis rather than \n",
    "real-time decision making.\n",
    "\n",
    "üìä METRIC EVALUATION:\n",
    "\n",
    "1. ACCURACY (Current: {:.1f}%):\n",
    "   - Measures overall correctness\n",
    "   - PRO: Easy to understand, good overall measure\n",
    "   - CON: Can be misleading with imbalanced data\n",
    "   - Our data: {:.1f}% non-survivors, {:.1f}% survivors\n",
    "\n",
    "2. PRECISION (Current: {:.1f}%):\n",
    "   - \"When we predict survival, how often are we correct?\"\n",
    "   - PRO: Important if false alarms are costly\n",
    "   - CON: May miss many actual survivors\n",
    "   - In our context: Reduces false hope to families\n",
    "\n",
    "3. RECALL/SENSITIVITY (Current: {:.1f}%):\n",
    "   - \"What percentage of actual survivors do we identify?\"\n",
    "   - PRO: Important if missing survivors is costly\n",
    "   - CON: May include many false alarms\n",
    "   - In our context: Ensures we identify most survivors\n",
    "\n",
    "4. F1-SCORE (Current: {:.1f}%):\n",
    "   - Harmonic mean of precision and recall\n",
    "   - PRO: Balanced view when both errors matter\n",
    "   - CON: Doesn't consider true negatives\n",
    "   - In our context: Good overall balance\n",
    "\n",
    "5. AUC-ROC (Current: {:.1f}%):\n",
    "   - Overall model performance across thresholds\n",
    "   - PRO: Robust to class imbalance, comprehensive\n",
    "   - CON: Doesn't give threshold-specific performance\n",
    "   - In our context: Good overall model assessment\n",
    "\n",
    "üí° BUSINESS IMPLICATIONS:\n",
    "\n",
    "‚Ä¢ FALSE POSITIVES (predict survival for deceased):\n",
    "  - Cost: Emotional distress to families, inaccurate historical record\n",
    "  - Impact: Medium - historical analysis context reduces immediate harm\n",
    "\n",
    "‚Ä¢ FALSE NEGATIVES (predict death for survivors):\n",
    "  - Cost: Missing recognition of survival, incomplete historical picture\n",
    "  - Impact: Medium - similar emotional/historical consequences\n",
    "\n",
    "üéØ MY CHOICE: RECALL is the most important metric\n",
    "\n",
    "WHY RECALL IS MOST IMPORTANT:\n",
    "\n",
    "1. HISTORICAL ACCURACY: In historical analysis, identifying all actual \n",
    "   survivors is crucial for accurate records and understanding what factors \n",
    "   truly contributed to survival.\n",
    "\n",
    "2. ETHICAL CONSIDERATION: While both error types have emotional impacts, \n",
    "   failing to recognize someone's survival seems more ethically problematic \n",
    "   than potentially misclassifying a non-survivor.\n",
    "\n",
    "3. RESEARCH VALUE: For understanding survival patterns, it's better to \n",
    "   capture all true survivors (high recall) even if we include some \n",
    "   false positives, as these can be filtered in further analysis.\n",
    "\n",
    "4. IMBALANCE HANDLING: With only {:.1f}% survivors, recall ensures we \n",
    "   don't overlook this important minority class.\n",
    "\n",
    "5. PRACTICAL TRADEOFF: In this historical context, the cost of missing \n",
    "   actual survivors (false negatives) outweighs the cost of false alarms \n",
    "   (false positives).\n",
    "\n",
    "SUPPORTING EVIDENCE:\n",
    "‚Ä¢ Our best model achieves {:.1f}% recall, meaning it identifies {:.1f}% \n",
    "  of actual survivors\n",
    "‚Ä¢ This comes with {:.1f}% precision, which is acceptable for historical analysis\n",
    "‚Ä¢ The F1-score of {:.1f}% shows good balance\n",
    "\n",
    "RECOMMENDATION:\n",
    "For the Titanic survival prediction project, prioritize RECALL while \n",
    "maintaining reasonable precision. Use F1-score as a secondary metric \n",
    "to ensure balance, and AUC for overall model selection.\n",
    "\n",
    "This approach ensures we capture the most historically valuable \n",
    "information while maintaining reasonable accuracy standards.\n",
    "\"\"\".format(\n",
    "    metrics_df['Accuracy'].max() * 100,\n",
    "    (y_test == 0).mean() * 100,\n",
    "    (y_test == 1).mean() * 100,\n",
    "    metrics_df['Precision'].max() * 100,\n",
    "    metrics_df['Recall'].max() * 100,\n",
    "    metrics_df['F1_Score'].max() * 100,\n",
    "    metrics_df['AUC'].max() * 100,\n",
    "    (y_test == 1).mean() * 100,\n",
    "    metrics_df['Recall'].max() * 100,\n",
    "    metrics_df['Recall'].max() * 100,\n",
    "    metrics_df['Precision'].max() * 100,\n",
    "    metrics_df['F1_Score'].max() * 100\n",
    "))\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b1446-fbd4-4da1-a28f-9325029222c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FINAL MODEL SELECTION AND RECOMMENDATIONS ===\")\n",
    "\n",
    "# Select best model based on chosen metric (Recall)\n",
    "best_model_name = metrics_df.loc[metrics_df['Recall'].idxmax(), 'Model']\n",
    "best_model_metrics = metrics_df[metrics_df['Model'] == best_model_name].iloc[0]\n",
    "\n",
    "print(f\"üéØ FINAL MODEL SELECTION: {best_model_name}\")\n",
    "print(f\"   Selected based on highest Recall (most important metric for our project)\")\n",
    "print(f\"\\nüìä BEST MODEL PERFORMANCE:\")\n",
    "print(f\"   Recall: {best_model_metrics['Recall']:.3f}\")\n",
    "print(f\"   Precision: {best_model_metrics['Precision']:.3f}\")\n",
    "print(f\"   F1-Score: {best_model_metrics['F1_Score']:.3f}\")\n",
    "print(f\"   AUC: {best_model_metrics['AUC']:.3f}\")\n",
    "\n",
    "print(f\"\\nüí° DEPLOYMENT RECOMMENDATIONS:\")\n",
    "print(\"1. Use this model for historical survival analysis\")\n",
    "print(\"2. Focus on interpreting factors that contribute to survival\")\n",
    "print(\"3. Consider the ethical implications of predictions\")\n",
    "print(\"4. Document limitations and uncertainty\")\n",
    "\n",
    "print(f\"\\nüöÄ FUTURE IMPROVEMENTS:\")\n",
    "print(\"‚Ä¢ Collect additional features if available\")\n",
    "print(\"‚Ä¢ Experiment with different algorithms\")\n",
    "print(\"‚Ä¢ Consider ensemble methods\")\n",
    "print(\"‚Ä¢ Perform cross-validation for more robust estimates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd958b1a-f581-4673-84ef-b2b25dcc81c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all evaluation results\n",
    "final_results = {\n",
    "    'project_name': 'Titanic Survival Prediction',\n",
    "    'best_model': best_model_name,\n",
    "    'most_important_metric': 'Recall',\n",
    "    'reasoning': 'Historical accuracy and ethical considerations prioritize identifying all actual survivors',\n",
    "    'final_metrics': best_model_metrics.to_dict(),\n",
    "    'all_models_comparison': metrics_df.to_dict('records')\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('final_evaluation_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "# Save best model separately\n",
    "best_model = models[best_model_name]\n",
    "joblib.dump(best_model, 'best_survival_model.pkl')\n",
    "\n",
    "print(\"üíæ FINAL RESULTS SAVED:\")\n",
    "print(\" - 'final_evaluation_results.json' (comprehensive evaluation)\")\n",
    "print(\" - 'best_survival_model.pkl' (best performing model)\")\n",
    "print(\" - 'comprehensive_metrics.csv' (detailed metrics comparison)\")\n",
    "print(f\"\\nüìÅ Save this notebook as 'week7_model_evaluation.ipynb'\")\n",
    "print(\"üöÄ Upload to GitHub to complete Assignment 7 and the 7-week course!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
