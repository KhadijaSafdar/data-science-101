{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349fa42-a698-4d6a-8d4c-ad5d275eaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 2: Data Cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ Week 2 Environment Ready!\")\n",
    "\n",
    "# Load your dataset (Titanic or your chosen dataset)\n",
    "# If using Titanic from local file:\n",
    "df = pd.read_csv(r'C:\\Users\\khadi\\Datasets\\titanic_large.csv')  # or your file path\n",
    "\n",
    "# If downloading fresh:\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa00e3e-788b-4cd9-8596-0f0a9ad858d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA ASSESSMENT (BEFORE CLEANING) ===\")\n",
    "\n",
    "print(\"\\n1. Dataset Overview:\")\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n2. First 10 rows:\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\n3. Dataset Information:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n4. Missing Values:\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "print(\"\\n5. Duplicate Rows:\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n6. Basic Statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b79f2f-dccf-4aec-aa75-677f8ef749dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== VISUAL ASSESSMENT ===\")\n",
    "\n",
    "# Plot missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_count = df.isnull().sum()\n",
    "missing_count[missing_count > 0].plot(kind='bar')\n",
    "plt.title('Missing Values Before Cleaning')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Save original dataset for comparison\n",
    "df_before = df.copy()\n",
    "print(\"‚úÖ Original dataset saved for comparison\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c289aa-9421-45bb-b347-ca1d5fb7a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== HANDLING MISSING VALUES ===\")\n",
    "\n",
    "# Strategy 1: Remove columns with too many missing values\n",
    "# If a column has more than 50% missing values, consider dropping\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "columns_to_drop = missing_percent[missing_percent > 50].index\n",
    "print(f\"Columns to consider dropping: {list(columns_to_drop)}\")\n",
    "\n",
    "# For Titanic dataset, Cabin has many missing values - we might drop it\n",
    "if 'Cabin' in df.columns:\n",
    "    df = df.drop('Cabin', axis=1)\n",
    "    print(\"Dropped 'Cabin' column (too many missing values)\")\n",
    "\n",
    "# Strategy 2: Fill numerical missing values with median\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "        print(f\"Filled missing values in {col} with median: {df[col].median()}\")\n",
    "\n",
    "# Strategy 3: Fill categorical missing values with mode\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mode_value = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "        df[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled missing values in {col} with mode: {mode_value}\")\n",
    "\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb7c50-4793-43d3-bcc9-ea96f1aa43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== HANDLING DUPLICATES ===\")\n",
    "\n",
    "duplicates_before = df.duplicated().sum()\n",
    "print(f\"Duplicate rows before: {duplicates_before}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "duplicates_after = df.duplicated().sum()\n",
    "print(f\"Duplicate rows after: {duplicates_after}\")\n",
    "print(f\"Removed {duplicates_before - duplicates_after} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b04d2-8ffa-4cf9-8ba5-5750143cbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== HANDLING OUTLIERS ===\")\n",
    "\n",
    "# Method 1: Identify outliers using IQR\n",
    "def detect_outliers_iqr(column):\n",
    "    Q1 = column.quantile(0.25)\n",
    "    Q3 = column.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = column[(column < lower_bound) | (column > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Check numerical columns for outliers\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_columns:\n",
    "    outliers = detect_outliers_iqr(df[col])\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"{col}: {len(outliers)} outliers detected\")\n",
    "        \n",
    "        # Visualize outliers\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        df[col].hist(bins=30)\n",
    "        plt.title(f'{col} - Distribution')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        df[col].plot(kind='box')\n",
    "        plt.title(f'{col} - Boxplot')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6484037-7c42-4181-9012-419f3a629ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA ASSESSMENT (AFTER CLEANING) ===\")\n",
    "\n",
    "print(f\"Dataset shape after cleaning: {df.shape}\")\n",
    "print(f\"Rows removed: {df_before.shape[0] - df.shape[0]}\")\n",
    "print(f\"Columns removed: {df_before.shape[1] - df.shape[1]}\")\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_after = df.copy()\n",
    "print(\"‚úÖ Cleaned dataset saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d421d-38f0-49d7-ab7f-22572b17ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä WEEK 2 ASSIGNMENT: BEFORE vs AFTER CLEANING REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Total Rows', 'Total Columns', 'Missing Values', 'Duplicate Rows'],\n",
    "    'Before Cleaning': [\n",
    "        df_before.shape[0],\n",
    "        df_before.shape[1],\n",
    "        df_before.isnull().sum().sum(),\n",
    "        df_before.duplicated().sum()\n",
    "    ],\n",
    "    'After Cleaning': [\n",
    "        df_after.shape[0],\n",
    "        df_after.shape[1],\n",
    "        df_after.isnull().sum().sum(),\n",
    "        df_after.duplicated().sum()\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        df_before.shape[0] - df_after.shape[0],\n",
    "        df_before.shape[1] - df_after.shape[1],\n",
    "        df_before.isnull().sum().sum() - df_after.isnull().sum().sum(),\n",
    "        df_before.duplicated().sum() - df_after.duplicated().sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "print(\"\\nüîç KEY CLEANING ACTIONS PERFORMED:\")\n",
    "cleaning_actions = []\n",
    "\n",
    "# Document what was done\n",
    "if 'Cabin' in df_before.columns and 'Cabin' not in df_after.columns:\n",
    "    cleaning_actions.append(\"Dropped 'Cabin' column (too many missing values)\")\n",
    "\n",
    "if df_before.duplicated().sum() > df_after.duplicated().sum():\n",
    "    cleaning_actions.append(f\"Removed {df_before.duplicated().sum() - df_after.duplicated().sum()} duplicate rows\")\n",
    "\n",
    "if df_before.isnull().sum().sum() > df_after.isnull().sum().sum():\n",
    "    cleaning_actions.append(f\"Handled {df_before.isnull().sum().sum() - df_after.isnull().sum().sum()} missing values\")\n",
    "\n",
    "for action in cleaning_actions:\n",
    "    print(f\"‚Ä¢ {action}\")\n",
    "\n",
    "print(f\"\\n‚úÖ DATA QUALITY IMPROVEMENT: {((df_before.isnull().sum().sum() - df_after.isnull().sum().sum()) / df_before.isnull().sum().sum() * 100):.1f}% reduction in missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096ec52-4396-49ff-aebb-515bd83c2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset to CSV\n",
    "df_after.to_csv('titanic_cleaned.csv', index=False)\n",
    "\n",
    "# Save the notebook\n",
    "print(\"üíæ Save this notebook as 'week2_data_cleaning.ipynb'\")\n",
    "print(\"üìÅ Upload to GitHub: week2_data_cleaning.ipynb + titanic_cleaned.csv\")\n",
    "print(\"üéØ Assignment 2 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
